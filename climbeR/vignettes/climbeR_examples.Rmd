---
title: "ClimbeR Examples"
author: "John Karlen"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{climbeR_examples}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Example Data

To illustrate an application of the climbeR package, I'll be using the "veteran" survival data set, and feeding it to the `ranger` R package; the result is a forest object that is ready to be climbed! (climbeRed?)

To start, lets make our forest
```{r, fig.show='hold'}
library(ranger)
require(survival)

# execute RSF (ranger), on the veteran data set
rg.veteran <- ranger(Surv(time, status) ~ .,
                     data = veteran,
                     write.forest = TRUE)

# retrieve result of variable importance (VIMP) for comparison later
vimp <- data.frame(rg.veteran$variable.importance)
```

The output of the call to `ranger` has a property `rg.veteran$forest`. This is a snapshot of the random forest, created by Ranger; it's the critical input to climbeR, which lets it calculate minimal maximal subtree depth.

## Using climbeR

Now that we have the result of a Random Survival Forest experiment, we can feed it to climbeR. The main function in climbeR is `getAndPlotSOvsFO`, which stands for "get and plot Second Order vs First Order"

```{r, include=FALSE}
library(climbeR)
# call to climber function
result <- getAndPlotSOvsFO(rg.veteran)
# ^ evaluated data ^
eval_data <- result$subtree_metrics
# second ord. vs first ord. plot
so_vs_fo <- result$plot
```
```{r, eval=FALSE}
library(climbeR)
# call to climber function
result <- getAndPlotSOvsFO(rg.veteran)
# evaluated data 
eval_data <- result$subtree_metrics
# second ord. vs first ord. plot
so_vs_fo <- result$plot
```
Lets take at the `$plot` property of the `result`, created above.
```{r, fig.width=6, fig.height=6}
# let's take a look
plot(so_vs_fo)
```

Variable importance in Random Forests is difficult to assess. One metric that provides some insight is the minimal depth at which a variable is used to split, on average, in the forest. Since each split in the tree subsets the data, splits near the root will affect larger portions of the data. In the plot above, we show first order (the highest split), and second order (the second highest split) average minimal maximal subtree depth. By this metric, the most important variables will occur in the bottom left corner, and the least important will occur at the top right. 

It is important to note that, although a variable has a large minimal maximal subtree depth (occurs in the top right corner), it may still be an important variable for the RSF. The subtree depth metric is biased towards continuous variables. For example, in the plot above, celltype probably has a larger first and second order depth, than karno and age. Karno and age are indeed predictive covariates, but they have high cardinality - so the metric will be biased towards them. It turns out that celltype has significant predictive information, but because it is a categorical variable, with low cardinality, it is not scored highly by this metric.

In the figure, dot size represents frequency with which the variable was used to split, in the forest. A boolean variable can only be used to split once because it is binary, so it's dot size will be small. A variable with high cardinality can be used to split many times. Karno and age, which have high cardinality, have larger points than celltype, prior or trt variables, which are categorical covariates, with low cardinality. The metric is biased towards these larger points, which is why celltype is scored significantly lower. 

We can also look at the results data in table form:

```{r, results='asis'}
# another look at the result 
knitr::kable(eval_data)
```

For more insight on minimal depth's bias towards continuous variables, we can add continuous noise to our dataset, and see if the metric will help us distinguish between noise, and real covariates.

First, lets make some noise. I'll define a simple function for adding n continuous, or discrete, noise covariates to a dataset:
```{r, include = FALSE}
# I'll use some dplyr goodness here, to mutate the df
library(dplyr)
```

```{r}

addNoise <- function(input_df, n, discrete = FALSE){
    df.len <- length(input_df[[1]])
    for(i in 1:n){
        #add a categorical noise variable w/ cardinality: 4
        noise_vect <- rnorm(df.len)
        if(discrete){
            #create RV w/ cardinality 4 (same as celltype)
            noise_vect <- sample(c(0, 1, 2, 3), df.len, replace = TRUE)
        }
        name <- paste0("n_", toString(i))
        input_df <- input_df %>% mutate(noise_vect)
        names(input_df)[length(input_df)] <- name
    }
    return(input_df)
}
```

Now we're ready to add some noise, and rerun the experiment

```{r, include=FALSE}
# make some noise!
veteran <- addNoise(veteran, 5)

# rerun RSF (ranger), on the veteran data set
rg.veteran <- ranger(Surv(time, status) ~ .,
                     data = veteran,
                     write.forest = TRUE)

# call to climber function
result <- getAndPlotSOvsFO(rg.veteran)
# ^ evaluated data ^
eval_data <- result$subtree_metrics
# second ord. vs first ord. plot
so_vs_fo <- result$plot
```
```{r, eval=FALSE}
# make some noise!
veteran <- addNoise(veteran, 5)

# rerun RSF (ranger), on the veteran data set
rg.veteran <- ranger(Surv(time, status) ~ .,
                     data = veteran,
                     write.forest = TRUE)

# call to climber function
result <- getAndPlotSOvsFO(rg.veteran)
# ^ evaluated data ^
eval_data <- result$subtree_metrics
# second ord. vs first ord. plot
so_vs_fo <- result$plot
```
Lets take at the plot:
```{r, fig.width=6, fig.height=6}
# let's take a look
plot(so_vs_fo)
```

The noise variables are labeled "n_#" 1 through 5. How do the continuous noise variables rank, compared to the categorical variables? They probably beat celltype, which is bad because we know the noise features contain no information! This is why subtree depth is not a sufficiently robust variable importance method for random forests.

To push the analysis a little further, we can see if the metric helps us distinguish between the predictive categorical variables, and some categorical variables generated from noise. I'll remove the continuous noise vars, just to keep the plot uncluttered, and then add 5 discrete noise covariates, each with cardinality .

```{r, include=FALSE}
# just removing the noise features by reloading the dataset
data(veteran)
# make some noise!
veteran <- addNoise(veteran, 5, discrete = TRUE)

# rerun RSF (ranger), on the veteran data set
rg.veteran <- ranger(Surv(time, status) ~ .,
                     data = veteran,
                     write.forest = TRUE)

# call to climber function
result <- getAndPlotSOvsFO(rg.veteran)
# ^ evaluated data ^
eval_data <- result$subtree_metrics
# second ord. vs first ord. plot
so_vs_fo <- result$plot
```
```{r, eval=FALSE}
# just removing the noise features by reloading the dataset
data(veteran)
# make some noise!
veteran <- addNoise(veteran, 5, discrete = TRUE)

# rerun RSF (ranger), on the veteran data set
rg.veteran <- ranger(Surv(time, status) ~ .,
                     data = veteran,
                     write.forest = TRUE)

# call to climber function
result <- getAndPlotSOvsFO(rg.veteran)
# ^ evaluated data ^
eval_data <- result$subtree_metrics
# second ord. vs first ord. plot
so_vs_fo <- result$plot
```

Lets take at the plot:
```{r, fig.width=6, fig.height=6}
# let's take a look
plot(so_vs_fo)
```

In your figure, celltype probably outranked all the categorical noise variables, of the same cardinality. This is great!
This speaks to the power of the metric to differentiate between covariates of similar cardinality.
